# LiteLLM Configuration for Tales of Charlie
# This file defines model routing, budget controls, and caching for AI integration

# General settings
general_settings:
  master_key: ${LITELLM_MASTER_KEY}
  database_url: ${LITELLM_DATABASE_URL} # Optional: for persistent budget tracking
  
# Model list - define available models and routing
model_list:
  - model_name: gpt-4o-mini
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: ${OPENAI_API_KEY}
      timeout: 30
      max_tokens: 2000
      temperature: 0.3
    
  - model_name: gpt-4o
    litellm_params:
      model: openai/gpt-4o
      api_key: ${OPENAI_API_KEY}
      timeout: 45
      max_tokens: 2000
      temperature: 0.3
    
  # Anthropic models (if API key provided)
  - model_name: claude-3-haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: ${ANTHROPIC_API_KEY}
      timeout: 30
      max_tokens: 2000
      temperature: 0.3
    model_info:
      mode: chat
      
  # Google models (if API key provided) 
  - model_name: gemini-pro
    litellm_params:
      model: gemini/gemini-pro
      api_key: ${GOOGLE_API_KEY}
      timeout: 30
      max_tokens: 2000
      temperature: 0.3

# Router settings
router_settings:
  routing_strategy: simple-shuffle # Load balance across available models
  model_group_alias: 
    conservative-eval: ["gpt-4o-mini", "claude-3-haiku"] # Models suitable for evaluation
    conservative-summary: ["gpt-4o-mini", "gpt-4o"] # Models for summary generation
  fallbacks:
    - model: gpt-4o-mini
      fallback: claude-3-haiku
    - model: claude-3-haiku  
      fallback: gpt-4o-mini
  
# Budget and rate limiting
budget_settings:
  max_budget: ${MAX_DAILY_COST_USD}
  budget_duration: "1d"
  budget_reset: "00:00:00"
  
# Rate limiting per model
rate_limit:
  gpt-4o-mini:
    requests_per_minute: 60
    tokens_per_minute: 50000
  gpt-4o:
    requests_per_minute: 30  
    tokens_per_minute: 30000
  claude-3-haiku:
    requests_per_minute: 40
    tokens_per_minute: 40000
    
# Caching configuration
cache_settings:
  type: "redis"
  host: "redis"
  port: 6379
  ttl: 1800 # Cache responses for 30 minutes
  
# Logging
logging_settings:
  log_level: "INFO"
  log_raw_request_response: false # Don't log content for privacy
  
# Health check endpoint
health_check_settings:
  healthy_endpoints: ["/health", "/health/liveliness", "/health/readiness"]
  
# Success callback (for usage tracking)
success_callback: ["budget_manager"]

# Failure callback (for error tracking)
failure_callback: ["budget_manager"]

# Environment-specific overrides
environment:
  production:
    logging_settings:
      log_level: "WARNING"
      log_raw_request_response: false
    cache_settings:
      ttl: 3600 # Longer cache in production (1 hour)
      
  development:
    logging_settings:
      log_level: "DEBUG"
    rate_limit:
      # More relaxed limits for development
      gpt-4o-mini:
        requests_per_minute: 100
        tokens_per_minute: 100000
